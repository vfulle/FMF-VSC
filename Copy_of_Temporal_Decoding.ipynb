{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.8 64-bit ('PRNI': conda)",
      "language": "python",
      "name": "python37864bitprniconda094311afa0894fd98fe9f5af3cee0992"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8-final"
    },
    "colab": {
      "name": "Copy of Temporal_Decoding.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vfulle/FMF-VSC/blob/master/Copy_of_Temporal_Decoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLINugJtDUgg",
        "colab_type": "text"
      },
      "source": [
        "# Multivariate Statistics (Decoding / MVPA) on MEG/EEG Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMC2tiixDUgh",
        "colab_type": "text"
      },
      "source": [
        "**Lecture:** Alexandre Gramfort\n",
        "\n",
        "**Tutorial:** Richard HÃ¶chenberger `<richard.hoechenberger@gmail.com>`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kWuO1qpDUgi",
        "colab_type": "text"
      },
      "source": [
        "## Objective\n",
        "\n",
        "This tutorial is concerned with the analysis of M/EEG data. It largely consists of two parts:\n",
        "\n",
        "- In the **first part,** you will learn how to **import raw data,** split it into **epochs,** and calculate **evoked responses.**\n",
        "- In the **second part,** we will **train a classifier** to distinguish the brain response patterns to two stimulus conditions, first on a **per-trial basis,** and then for **every time point.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKajRwyaDUgj",
        "colab_type": "text"
      },
      "source": [
        "Install MNE-Python if it has not yet been installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgXC40blDUgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-NMhQyEDUgq",
        "colab_type": "text"
      },
      "source": [
        "First, load the mne package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT5aAg83DUgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import mne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTJt5TEBDUgv",
        "colab_type": "text"
      },
      "source": [
        "## Load the raw data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW-7ryQHDUgw",
        "colab_type": "text"
      },
      "source": [
        "Now we load the `sample` dataset. It will be downloaded automatically (approx. 2 GB)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "CLdWIF4rDUgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# data_path() returns a string â€“ turn it into a Path!\n",
        "sample_dir = Path(mne.datasets.sample.data_path())\n",
        "\n",
        "# The raw data file we wish to load â€“Â an example experiment.\n",
        "raw_fname = sample_dir / 'MEG/sample/sample_audvis_filt-0-40_raw.fif'\n",
        "\n",
        "# Now, actually load the raw data.\n",
        "raw = mne.io.read_raw_fif(raw_fname, preload=True)\n",
        "print(raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLVzR4rJDUgz",
        "colab_type": "text"
      },
      "source": [
        "## High-pass filter the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "5MOMd-r7DUg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw.filter(l_freq=1, h_freq=None, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN3xq0_7DUg2",
        "colab_type": "text"
      },
      "source": [
        "## View some metadata & information on the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "0CxnBWL3DUg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(raw.info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL96JqSuDUg5",
        "colab_type": "text"
      },
      "source": [
        "## Extract experimental events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBbosgAXDUg6",
        "colab_type": "text"
      },
      "source": [
        "First extract all experimental events. In this particular dataset, they are stored in the stimulus or trigger channel `STI 014`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "LtxiiA8VDUg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "events = mne.find_events(raw, stim_channel='STI 014', verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaP1GnGkDUg9",
        "colab_type": "text"
      },
      "source": [
        "Look at the design in a graphical way. We're only interested in the `auditory left` and `auditory right` conditions, encoded through event IDs `1` and `2`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JxhFqn7DUg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "event_id = { 'aud_l': 1,\n",
        "             'aud_r': 2 } \n",
        "\n",
        "fig = mne.viz.plot_events(events, sfreq=raw.info['sfreq'],\n",
        "                          first_samp=raw.first_samp, event_id=event_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDHcprszDUhB",
        "colab_type": "text"
      },
      "source": [
        "## Epoch the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_ziQ4aWDUhB",
        "colab_type": "text"
      },
      "source": [
        "Now that we know when the experimental events of interest happened, we are finally ready to slice the data into epochs!\n",
        "\n",
        "Note that, since we high-pass filtered the raw data, we do not need to perform baseline correction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "jqQF_EX-DUhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmin = -0.1  # Start of each epoch relative to the event marker (in seconds)\n",
        "tmax = 0.4   # End of each epoch\n",
        "baseline = None  # No baseline correction, data were high-pass filtered!\n",
        "\n",
        "# Define rejection parameters: epochs with unusually high amplitudes should be dropped\n",
        "# automatically.\n",
        "reject = dict(eeg=100e-6, eog=200e-6)  # Amplitude threshold in Volts.\n",
        "\n",
        "# Now actually create the epochs.\n",
        "epochs = mne.Epochs(raw, tmin=tmin, tmax=tmax,\n",
        "                    events=events, event_id=event_id,\n",
        "                    baseline=baseline, reject=reject,\n",
        "                    preload=True)  # Actually load the data into memory.\n",
        "\n",
        "print(epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TNU7Cc8DUhD",
        "colab_type": "text"
      },
      "source": [
        "## Calculate evoked responses (ERF & ERP)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLa09CJjDUhD",
        "colab_type": "text"
      },
      "source": [
        "Display the evoked responses and visualize the differences between left and right stimulation. We use \"spatial\" colors for the traces, which encode the sensor location (legend shown in the top-left corner of the figures)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBlKwQIwDUhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evoked_left = epochs['aud_l'].average()\n",
        "evoked_right = epochs['aud_r'].average()\n",
        "\n",
        "# Constrast = Left - Right stimulation.\n",
        "evoked_contrast = mne.combine_evoked([evoked_left, evoked_right],\n",
        "                                     weights=[1, -1])\n",
        "\n",
        "# Plot the results. The only reason we're using show=False here is\n",
        "# because otherwise, Jupyter Notebook sometimes may produce multiple\n",
        "# figures per plotting command. Otherwise, this should not be\n",
        "# necessary.\n",
        "fig = evoked_left.plot(spatial_colors=True, show=False)\n",
        "fig.suptitle('Left')\n",
        "\n",
        "fig = evoked_right.plot(spatial_colors=True, show=False)\n",
        "fig.suptitle('Right')\n",
        "\n",
        "fig = evoked_contrast.plot(spatial_colors=True, show=False)\n",
        "fig.suptitle('Left â€“ Right')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHhMRSNfDUhF",
        "colab_type": "text"
      },
      "source": [
        "## Plot some topographies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "630bBdIxDUhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vmin, vmax = -4, 4  # Colorbar range â€“ set to None, None for automatic scaling.\n",
        "fig = evoked_left.plot_topomap(ch_type='eeg', contours=0, vmin=vmin, vmax=vmax,\n",
        "                               show=False)\n",
        "fig.suptitle('Left')\n",
        "\n",
        "fig = evoked_right.plot_topomap(ch_type='eeg', contours=0, vmin=vmin, vmax=vmax,\n",
        "                                show=False)\n",
        "fig.suptitle('Right')\n",
        "\n",
        "# For the contrast, we allow MNE to scale the colorbar automatically.\n",
        "fig = evoked_contrast.plot_topomap(ch_type='eeg', contours=0, show=False)\n",
        "fig.suptitle('Left â€“ Right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w36Cnu5UDUhI",
        "colab_type": "text"
      },
      "source": [
        "## Now let's see if we can classify single trials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET2oW89KDUhI",
        "colab_type": "text"
      },
      "source": [
        "### Equalize the number of epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPVMAHIoDUhI",
        "colab_type": "text"
      },
      "source": [
        "To keep chance level at 50% accuracy, we first equalize the number of epochs in each condition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "njaEU3JbDUhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs.equalize_event_counts(event_id)\n",
        "print(epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hStxLWYxDUhK",
        "colab_type": "text"
      },
      "source": [
        "### Create input `X` and response `y`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJMkW3BgDUhL",
        "colab_type": "text"
      },
      "source": [
        "A classifier takes as input a matrix `X` and returns a vector `y` (consisting of `0` and `1`). Here `X` will be the **data at one time point on all gradiometers** (hence the term multivariate). We want to train our model to discriminate between the  `auditory left` and the `auditory right` trials.\n",
        "\n",
        "We work with all sensors jointly and try to find a discriminative pattern between the two conditions to predict the experimental condition of individual trials.\n",
        "\n",
        "For classification we will use the `scikit-learn` package (http://scikit-learn.org/) and MNE-Python functions.\n",
        "\n",
        "Let's first create the response vector, `y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "89b9haCADUhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create an vector with length = no. of trials.\n",
        "y = np.empty(len(epochs.events), dtype=int)  \n",
        "\n",
        "# Which trials are LEFT, which are RIGHT?\n",
        "idx_auditory_left = epochs.events[:, 2] == event_id['aud_l']\n",
        "idx_auditory_right = epochs.events[:, 2] == event_id['aud_r']\n",
        "\n",
        "# Encode: LEFT = 0, RIGHT = 1.\n",
        "y[idx_auditory_left] = 0\n",
        "y[idx_auditory_right] = 1\n",
        "\n",
        "print(y)\n",
        "print(f'\\nSize of y: {y.size}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHc55EzrDUhN",
        "colab_type": "text"
      },
      "source": [
        "Now, let's create the input matrix, `X`.\n",
        "\n",
        "We wish to focus only on the gradiometer channels here, so we use\n",
        "`pick_types(meg='grad')`. For magnetometer channels, we would need to\n",
        "pass `meg='mag'`; and for EEG channels: `meg=False, eeg=True`.\n",
        "We create a copy of the `epochs` because `pick_types()` operates in-place,\n",
        "but we would like to keep the original epochs object untouched.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "p9FyYOzWDUhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs_grad = epochs.copy().pick_types(meg='grad')\n",
        "\n",
        "# Retrieve the data as a NumPy array.\n",
        "# The array has the shape: (n_trials, n_channels, n_timepoints)\n",
        "data = epochs_grad.get_data()\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZQrNSlfDUhP",
        "colab_type": "text"
      },
      "source": [
        "We're almost there! We need to reshape the array such that for each trial, we have a vector `[channel_1_time_1, channel_1_time_2, ..., channel_m_time_n]`, i.e., we aim to reshape `X` to the dimension `(n_trials, n_channels * n_timepoints)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "5KGdNduEDUhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_trials = data.shape[0]\n",
        "\n",
        "X = data.reshape(n_trials, -1)\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3dinG8fDUhR",
        "colab_type": "text"
      },
      "source": [
        "### Create a classifier!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZuppB4nDUhR",
        "colab_type": "text"
      },
      "source": [
        "We will use plain `scikit-learn` machinery for the first round of classifications. This is to demonstrate that you can simply feed pre-processed data from MNE into `scikit-learn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "wo2OeSqrDUhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "# The classifier pipeline: it is extremely important to scale the data\n",
        "# before running the actual classifier (logistic regression in our case).\n",
        "clf = make_pipeline(StandardScaler(),\n",
        "                    LogisticRegression())\n",
        "\n",
        "# Run cross-validation.\n",
        "# CV without shuffling â€“ \"block cross-validation\" â€“Â is what we want here\n",
        "# (scikit-learn doesn't shuffle by default, which is good for us).\n",
        "n_splits = 5\n",
        "scoring = 'roc_auc'\n",
        "cv = StratifiedKFold(n_splits=n_splits)\n",
        "scores = cross_val_score(clf, X=X, y=y, cv=cv, scoring=scoring)\n",
        "\n",
        "# Mean and standard deviation of ROC AUC across cross-validation runs.\n",
        "roc_auc_mean = round(np.mean(scores), 3)\n",
        "roc_auc_std = round(np.std(scores), 3)\n",
        "\n",
        "print(f'CV scores: {scores}')\n",
        "print(f'Mean ROC AUC = {roc_auc_mean:.3f} (SD = {roc_auc_std:.3f})')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1PMZr3NDUhU",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the cross-validation results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynExxxaoDUhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.boxplot(scores,\n",
        "           showmeans=True, # Green triangle marks the mean.\n",
        "           whis=(0, 100),  # Whiskers span the entire range of the data.\n",
        "           labels=['Left vs Right'])\n",
        "ax.set_xlabel('Score')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_title('Cross-Validation Scores')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnSU2hnrDUhV",
        "colab_type": "text"
      },
      "source": [
        "## We can do this more simply using the `mne.decoding` module! Let's go. ðŸš€"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "zeL3Rke_DUhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from mne.decoding import Scaler, Vectorizer, cross_val_multiscore\n",
        "\n",
        "# First, create X and y.\n",
        "epochs_grad = epochs.copy().pick_types(meg='grad')\n",
        "X = epochs_grad.get_data()\n",
        "y = epochs_grad.events[:, 2]\n",
        "\n",
        "# Classifier pipeline.\n",
        "clf = make_pipeline(\n",
        "    # An MNE scaler that correctly handles different channel types â€“\n",
        "    # isn't that great?!\n",
        "    Scaler(epochs_grad.info),\n",
        "    # Remember this annoying and error-prone NumPy array reshaping we had to do\n",
        "    # earlier? Not anymore, thanks to the MNE vectorizer!\n",
        "    Vectorizer(),\n",
        "    # And, finally, the actual classifier.\n",
        "    LogisticRegression())\n",
        "\n",
        "# Run cross-validation.\n",
        "# Note that we're using MNE's cross_val_multiscore() here, not scikit-learn's\n",
        "# cross_val_score() as above. We simply pass the number of desired CV splits,\n",
        "# and MNE will automatically do the rest for us.\n",
        "n_splits = 5\n",
        "scoring = 'roc_auc'\n",
        "scores = cross_val_multiscore(clf, X, y, cv=5, scoring='roc_auc')\n",
        "\n",
        "# Mean and standard deviation of ROC AUC across cross-validation runs.\n",
        "roc_auc_mean = round(np.mean(scores), 3)\n",
        "roc_auc_std = round(np.std(scores), 3)\n",
        "\n",
        "print(f'CV scores: {scores}')\n",
        "print(f'Mean ROC AUC = {roc_auc_mean:.3f} (SD = {roc_auc_std:.3f})')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztaurvx-DUhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1feZq2SDUhZ",
        "colab_type": "text"
      },
      "source": [
        "## Decoding over time: Comparisons at every single time point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgq36jtpDUhZ",
        "colab_type": "text"
      },
      "source": [
        "In the previous examples, we have trained a classifier to discriminate between experimental conditions by using the spatio-temporal patterns of **entire trials**. Consequently, the classifier was (hopefully!) able to predict which activation patterns belonged to which condition. \n",
        "\n",
        "However, an interesting neuroscientific is: **Exactly *when* do the brain signals for two conditions differ?**\n",
        "\n",
        "We can try to answer this question by fitting a classifier **at every single time point.** If the classifier can successfully discriminate between the two conditions, we can conclude that the spatial activation patterns measured by the MEG or EEG sensors differed **at this very time point**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "Dn-Lhu37DUha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from mne.decoding import SlidingEstimator\n",
        "\n",
        "# First, create X and y.\n",
        "epochs_grad = epochs.copy().pick_types(meg='grad')\n",
        "X = epochs_grad.get_data()\n",
        "y = epochs_grad.events[:, 2]\n",
        "\n",
        "# Classifier pipeline. No need for vectorization as in the previous example.\n",
        "clf = make_pipeline(StandardScaler(),\n",
        "                    LogisticRegression())\n",
        "\n",
        "# The \"sliding estimator\" will train the classifier at each time point.\n",
        "scoring = 'roc_auc'\n",
        "time_decoder = SlidingEstimator(clf, scoring=scoring, n_jobs=1, verbose=True)\n",
        "\n",
        "# Run cross-validation.\n",
        "n_splits = 5\n",
        "scores = cross_val_multiscore(time_decoder, X, y, cv=5, n_jobs=1)\n",
        "\n",
        "# Mean scores across cross-validation splits, for each time point.\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "\n",
        "# Mean score across all time points.\n",
        "mean_across_all_times = round(np.mean(scores), 3)\n",
        "print(f'\\n=> Mean CV score across all time points: {mean_across_all_times:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3fM6QZeDUhb",
        "colab_type": "text"
      },
      "source": [
        "### Plot the classification results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbV-N21TDUhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(epochs.times, mean_scores, label='score')\n",
        "ax.axhline(0.5, color='k', linestyle='--', label='chance')  # AUC = 0.5\n",
        "ax.axvline(0, color='k', linestyle='-')  # Mark time point zero.\n",
        "\n",
        "ax.set_xlabel('Time (s)')\n",
        "ax.set_ylabel('Mean ROC AUC')\n",
        "ax.legend()\n",
        "ax.set_title('Left vs Right')\n",
        "fig.suptitle('Sensor Space Decoding')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcesBjYgDUhc",
        "colab_type": "text"
      },
      "source": [
        "---------------\n",
        "\n",
        "## Exercise I\n",
        "\n",
        "- In the above example, we only used gradiometer (`grad`) channels. The sample data, however, also contains magnetometer (`mag`) and EEG (`eeg`) channels. Which sensor types produce the best classification scores?\n",
        "\n",
        "- How does classification performance change if you alter the preprocessing parameters (filter cutoff frequencies, rejection parameters during epoching)?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rPgHDO4DUhc",
        "colab_type": "text"
      },
      "source": [
        "## Exercise II \n",
        "\n",
        "The SPM `face` dataset shipping with MNE-Python contains MEG recordings from one participant who was presented with faces and scrambled faces in randomized order.\n",
        "\n",
        "- Do a decoding over time on the SPM `face` dataset to see if you can classify `face` vs. `scrambled face`.\n",
        "\n",
        "Hints:\n",
        "\n",
        "- Access the `face` dataset via:\n",
        "\n",
        "    ```\n",
        "    sample_dir = Path(mne.datasets.spm_face.data_path())  # Downloads data.\n",
        "    # Use str() here to work around a bug in MNE.\n",
        "    raw_fname = str(sample_dir / 'MEG/spm/SPM_CTF_MEG_example_faces1_3D.ds')\n",
        "    raw = mne.io.read_raw_ctf(raw_fname, preload=True)\n",
        "    ```\n",
        "\n",
        "- The event IDs are:\n",
        "\n",
        "    ```\n",
        "    event_ids = {'faces': 1, 'scrambled': 2}\n",
        "    ```\n",
        "\n",
        "- The trigger channel is `UPPT001`.\n",
        "\n",
        "- Consider creating slightly longer epochs than for the auditory task.\n",
        "\n",
        "- Consider setting an upper frequency limit for the filter which is well below the line noise frequency, e.g. `filter(l_freq=..., h_freq=40)`.\n",
        "\n",
        "See this online example for additional hints: https://mne.tools/dev/auto_examples/datasets/spm_faces_dataset.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZT0w4nxDUhd",
        "colab_type": "text"
      },
      "source": [
        "### Example Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "tYHQfFrkDUhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import mne\n",
        "from mne.decoding import SlidingEstimator, cross_val_multiscore\n",
        "\n",
        "\n",
        "# Load raw data.\n",
        "data_path = Path(mne.datasets.spm_face.data_path())\n",
        "raw_fname = str(data_path / 'MEG/spm/SPM_CTF_MEG_example_faces1_3D.ds')\n",
        "raw = mne.io.read_raw_ctf(raw_fname, preload=True)\n",
        "\n",
        "# High-pass filter the data.\n",
        "raw.filter(l_freq=1, h_freq=40, verbose=True)\n",
        "\n",
        "# Find events.\n",
        "events = mne.find_events(raw, stim_channel='UPPT001', verbose=True)\n",
        "\n",
        "# Create epochs.\n",
        "event_id = {'faces': 1,\n",
        "            'scrambled': 2}\n",
        "tmin = -0.2  # Start of each epoch relative to the event marker (in seconds)\n",
        "tmax = 0.6   # End of each epoch\n",
        "baseline = None  # No baseline correction, data were high-pass filtered!\n",
        "epochs = mne.Epochs(raw, tmin=tmin, tmax=tmax,\n",
        "                    events=events, event_id=event_id,\n",
        "                    baseline=baseline,\n",
        "                    preload=True)  # Actually load the data into memory.\n",
        "print(epochs)\n",
        "\n",
        "# We don't need to equalize the number of trials, as we have the same number\n",
        "# for each condition.\n",
        "\n",
        "# Create X and y.\n",
        "epochs_grad = epochs.copy().pick_types(meg='grad')\n",
        "X = epochs_grad.get_data()\n",
        "y = epochs_grad.events[:, 2]\n",
        "\n",
        "# Classifier pipeline.\n",
        "clf = make_pipeline(StandardScaler(),\n",
        "                    LogisticRegression())\n",
        "\n",
        "# The \"sliding estimator\" will train the classifier at each time point.\n",
        "scoring = 'roc_auc'\n",
        "time_decoder = SlidingEstimator(clf, scoring=scoring, n_jobs=1, verbose=True)\n",
        "\n",
        "# Run cross-validation.\n",
        "n_splits = 5\n",
        "scores = cross_val_multiscore(time_decoder, X, y, cv=5, n_jobs=1)\n",
        "\n",
        "# Mean scores across cross-validation splits, for each time point.\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "\n",
        "# Mean score across all time points.\n",
        "mean_across_all_times = round(np.mean(scores), 3)\n",
        "print(f'\\n=> Mean CV score across all time points: {mean_across_all_times:.3f}')\n",
        "\n",
        "# Plot scores.\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(epochs.times, mean_scores, label='score')\n",
        "ax.axhline(0.5, color='k', linestyle='--', label='chance')  # AUC = 0.5\n",
        "ax.axvline(0, color='k', linestyle='-')  # Mark time point zero.\n",
        "\n",
        "ax.set_xlabel('Time (s)')\n",
        "ax.set_ylabel('Mean ROC AUC')\n",
        "ax.legend()\n",
        "ax.set_title('Faces vs Scrambled Faces')\n",
        "fig.suptitle('Sensor Space Decoding')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZG_O2adDUhe",
        "colab_type": "text"
      },
      "source": [
        "## Bonus Task\n",
        "\n",
        "Temporal generalization or \"generalization over time\" (GAT) is an extension of the \"decoding over time\" approach we used above. It evaluates **whether the model estimated at a particular time instant accurately predicts any other time instant.**\n",
        "\n",
        "Perform a GAT analysis as explained in the [MNE documentation on decoding](https://mne.tools/stable/auto_tutorials/machine-learning/plot_sensors_decoding.html#temporal-generalization).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOqs8WLmDUhf",
        "colab_type": "text"
      },
      "source": [
        "-------\n",
        "\n",
        "*For more information on decoding, see: https://mne.tools/stable/auto_tutorials/machine-learning/plot_sensors_decoding.html and this book chapter: Jean-RÃ©mi King, Laura Gwilliams, Chris Holdgraf, Jona Sassenhagen, Alexandre Barachant, Denis Engemann, Eric Larson, Alexandre Gramfort. Encoding and Decoding Neuronal Dynamics: Methodological Framework to Uncover the Algorithms of Cognition. 2018. https://hal.archives-ouvertes.fr/hal-01848442/*"
      ]
    }
  ]
}